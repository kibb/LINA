{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaV4mHBXVj2x"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.layers import Input , Conv2D , MaxPooling2D , Dropout , concatenate , UpSampling2D, MaxPooling3D, Conv3D, Reshape, Activation\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Define paths for the test set and the pre-trained model\n",
    "trainingSetPath = 'Training Set' #Please modify the path if needed\n",
    "testSetPath = 'Test Set' #Please modify the path if needed\n",
    "modelPath = 'Models' #Please modify the path if needed\n",
    "savePath = 'Saved Models' #Please modify the path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a U-Net model\n",
    "def UNet(input_shape):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Input\n",
    "    input_layer = Input(input_shape)\n",
    "    nb_kernels = 16\n",
    "\n",
    "    # Downsampling path\n",
    "    conv1 = Conv3D(nb_kernels, (3, 3, 8), activation='relu', padding='same', kernel_initializer='he_normal')(input_layer)\n",
    "    conv1 = Conv3D(nb_kernels, (3, 3, 8), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 8))(conv1)\n",
    "    pool1_reshaped = Reshape((176, 176, nb_kernels))(pool1)\n",
    "    conv1_upsampled = Reshape((352, 352, nb_kernels * 8))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool1_reshaped)\n",
    "    conv2 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    # Upsampling path\n",
    "    conv5 = Conv2D(nb_kernels * 16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(nb_kernels * 16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(nb_kernels * 8, (2, 2), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(nb_kernels * 2, (2, 2), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(nb_kernels, (2, 2), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1_upsampled, up9], axis=3)\n",
    "    conv9 = Conv2D(nb_kernels, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(nb_kernels, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(1, 1, activation=tf.keras.activations.linear)(conv9)\n",
    "\n",
    "    # Create and compile the U-Net model\n",
    "    model = keras.Model(inputs=input_layer, outputs=outputs, name='UNet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 647.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 233.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load training images for Fluorescence\n",
    "image_list = []\n",
    "listOfImages=glob.glob(trainingSetPath + '/Fluorescence/*.tif')\n",
    "imagesLength=len(listOfImages)\n",
    "for ii in tqdm(range(0,imagesLength)):\n",
    "    im=plt.imread(listOfImages[ii], 'tif')\n",
    "    im_max = np.max(im) #for normalization\n",
    "    im_min = np.min(im) #for normalization\n",
    "    if(im_max != 0):\n",
    "        im = (im - im_min)/(im_max - im_min)\n",
    "    image_list.append(im)\n",
    "labels = np.array(image_list)\n",
    "image_list = None\n",
    "\n",
    "# Load training images for Phase\n",
    "image_list = []\n",
    "listOfImages=glob.glob(trainingSetPath + '/Phase/*.tif')\n",
    "imagesLength=len(listOfImages)\n",
    "for ii in tqdm(range(0,imagesLength)):\n",
    "    im = io.imread(listOfImages[ii])\n",
    "    for jj in range(0,8):\n",
    "        im_max = np.max(im[jj]) #for normalization\n",
    "        im_min = np.min(im[jj]) #for normalization\n",
    "        im[jj] = (im[jj] - im_min)/(im_max - im_min)\n",
    "    image_list.append(im)\n",
    "inputImages = np.array(image_list)\n",
    "inputImages = np.swapaxes(inputImages, 1, 2)\n",
    "inputImages = np.swapaxes(inputImages, 2, 3)\n",
    "image_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into a test set + trainining/validation set\n",
    "nOfImages=len(labels)\n",
    "testSize=int(0.9*nOfImages)\n",
    "\n",
    "train_images=inputImages[0:testSize]\n",
    "train_labels=labels[0:testSize]\n",
    "\n",
    "test_images=inputImages[testSize:]\n",
    "test_labels=labels[testSize:]\n",
    "\n",
    "# Reshape training and test images\n",
    "train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], train_images.shape[3], 1)\n",
    "train_labels = train_labels.reshape(train_labels.shape[0], train_labels.shape[1], train_labels.shape[2], 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], test_images.shape[1], test_images.shape[2], test_images.shape[3], 1)\n",
    "test_labels = test_labels.reshape(test_labels.shape[0], test_labels.shape[1], test_labels.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "transfer_model = UNet((352, 352, 8, 1))\n",
    "transfer_model.load_weights(modelPath + '/PixelRegressionModel.h5')\n",
    
    "\n",
    "# Define the ratio of validation data to training data for the new dataset\n",
    "validtrain_split_ratio = 0.2  # % of the seen dataset to be put aside for validation, rest is for training\n",
    "\n",
    "# Specify whether to shuffle the training data before each epoch\n",
    "batch_shuffle = True   # shuffle the training data prior to batching before each epoch\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "lrate = 1e-4\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 500\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 10\n",
    "\n",
    "# Define callback functions\n",
    "my_callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=30, verbose=1, mode='auto'),\n",
    "    \n",
    "    # Save the best model checkpoints during training\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath= savePath + '/model_{epoch:02d}_{val_loss:.5f}.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto'),\n",
    "]\n",
    "\n",
    "# Compile the transfer model with a new loss function and optimizer\n",
    "transfer_model.compile(\n",
    "    optimizer=optimizers.Adam(lr=lrate),  # You can adjust the learning rate\n",
    "    loss='mean_squared_error',  # Use an appropriate loss function for your task\n",
    "    metrics=['mean_absolute_error']  # Add relevant metrics\n",
    ")\n",
    "\n",
    "# Train the transfer model on the new dataset\n",
    "transfer_history = transfer_model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks = my_callbacks ,\n",
    "    validation_split=validtrain_split_ratio,\n",
    "    shuffle=batch_shuffle,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, you can use the transfer_model for predictions on your new images\n",
    "new_test_predict = transfer_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model loss\n",
    "\n",
    "plt.plot(transfer_history.transfer_history['loss'])\n",
    "plt.plot(transfer_history.transfer_history['val_loss'])\n",
    "plt.ylabel('Loss [MSE]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# plot the model accuracy metric\n",
    "metrics = ['mean_absolute_error']\n",
    "plt.plot(np.array(transfer_history.transfer_history[metrics[0]]))\n",
    "plt.plot(np.array(transfer_history.transfer_history['val_' + metrics[0]]))\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xsMY2VZgywF8"
   ],
   "name": "Copy of ATS_Classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
