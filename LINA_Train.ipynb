{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaV4mHBXVj2x"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.layers import Input , Conv2D , MaxPooling2D , Dropout , concatenate , UpSampling2D, MaxPooling3D, Conv3D, Reshape, Activation\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Define paths for the test set and the pre-trained model\n",
    "trainingSetPath = 'Training Set' #Please modify the path if needed\n",
    "savePath = 'Saved Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a U-Net model\n",
    "def UNet(input_shape):\n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    # Input\n",
    "    input_layer = Input(input_shape)\n",
    "    nb_kernels = 16\n",
    "\n",
    "    # Downsampling path\n",
    "    conv1 = Conv3D(nb_kernels, (3, 3, 8), activation='relu', padding='same', kernel_initializer='he_normal')(input_layer)\n",
    "    conv1 = Conv3D(nb_kernels, (3, 3, 8), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 8))(conv1)\n",
    "    pool1_reshaped = Reshape((176, 176, nb_kernels))(pool1)\n",
    "    conv1_upsampled = Reshape((352, 352, nb_kernels * 8))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool1_reshaped)\n",
    "    conv2 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    # Upsampling path\n",
    "    conv5 = Conv2D(nb_kernels * 16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(nb_kernels * 16, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(nb_kernels * 8, (2, 2), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(drop5))\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(nb_kernels * 8, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(nb_kernels * 4, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(nb_kernels * 2, (2, 2), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
    "    merge8 = concatenate([conv2, up8], axis=3)\n",
    "    conv8 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(nb_kernels * 2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(nb_kernels, (2, 2), activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
    "    merge9 = concatenate([conv1_upsampled, up9], axis=3)\n",
    "    conv9 = Conv2D(nb_kernels, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(nb_kernels, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Conv2D(1, 1, activation=tf.keras.activations.linear)(conv9)\n",
    "\n",
    "    # Create and compile the U-Net model\n",
    "    model = keras.Model(inputs=input_layer, outputs=outputs, name='UNet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 659.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 217.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load training images for Fluorescence\n",
    "image_list = []\n",
    "listOfImages=glob.glob(trainingSetPath + '/Fluorescence/*.tif')\n",
    "imagesLength=len(listOfImages)\n",
    "for ii in tqdm(range(0,imagesLength)):\n",
    "    im=plt.imread(listOfImages[ii], 'tif')\n",
    "    im_max = np.max(im) #for normalization\n",
    "    im_min = np.min(im) #for normalization\n",
    "    if(im_max != 0):\n",
    "        im = (im - im_min)/(im_max - im_min)\n",
    "    image_list.append(im)\n",
    "train_labels = np.array(image_list)\n",
    "image_list = None\n",
    "\n",
    "# Load training images for Phase\n",
    "image_list = []\n",
    "listOfImages=glob.glob(trainingSetPath + '/Phase/*.tif')\n",
    "imagesLength=len(listOfImages)\n",
    "for ii in tqdm(range(0,imagesLength)):\n",
    "    im = io.imread(listOfImages[ii])\n",
    "    for jj in range(0,8):\n",
    "        im_max = np.max(im[jj]) #for normalization\n",
    "        im_min = np.min(im[jj]) #for normalization\n",
    "        im[jj] = (im[jj] - im_min)/(im_max - im_min)\n",
    "    image_list.append(im)\n",
    "train_images = np.array(image_list)\n",
    "train_images = np.swapaxes(train_images, 1, 2)\n",
    "train_images = np.swapaxes(train_images, 2, 3)\n",
    "image_list = None\n",
    "\n",
    "# Reshape training images\n",
    "train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], train_images.shape[3], 1)\n",
    "train_labels = train_labels.reshape(train_labels.shape[0], train_labels.shape[1], train_labels.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a U-Net model with input shape (352, 352, 8)\n",
    "model = UNet((352,352,8, 1))\n",
    "\n",
    "# Define the ratio of validation data to training data\n",
    "validtrain_split_ratio = 0.2  # % of the seen dataset to be put aside for validation, rest is for training\n",
    "\n",
    "# Specify whether to shuffle the training data before each epoch\n",
    "batch_shuffle = True   # shuffle the training data prior to batching before each epoch\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "lrate = 1e-4\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 500\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 10\n",
    "\n",
    "# Define callback functions for training\n",
    "my_callbacks = [\n",
    "    # Early stopping to prevent overfitting\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=30, verbose=1, mode='auto'),\n",
    "    \n",
    "    # Save the best model checkpoints during training\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=savePath + '/model_{epoch:02d}_{val_loss:.5f}.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto'),\n",
    "]\n",
    "\n",
    "# Compile the model with specified optimizer, loss function, and metrics\n",
    "model.compile(optimizer = optimizers.Adam(lrate),\n",
    "              loss = 'mean_squared_error',\n",
    "              metrics = ['mean_absolute_error'])\n",
    "\n",
    "# Train the model using fit() function\n",
    "history = model.fit(train_images, train_labels, \n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs , callbacks = my_callbacks ,\n",
    "                    validation_split=validtrain_split_ratio,\n",
    "                    shuffle=batch_shuffle,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model loss\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss [MSE]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "# plot the model accuracy metric\n",
    "metrics = ['mean_absolute_error']\n",
    "plt.plot(np.array(history.history[metrics[0]]))\n",
    "plt.plot(np.array(history.history['val_' + metrics[0]]))\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xsMY2VZgywF8"
   ],
   "name": "Copy of ATS_Classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
